{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Word_embeddings.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNLfcw99XC8fm7BGov15Jfv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/smf-9000/nlp-in-general/blob/main/Word_embeddings.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "US1O6x2b6Stb"
      },
      "source": [
        "\n",
        "\n",
        "```\n",
        "Links:\n",
        "[Chris McCormick and Nick Ryan. (2019, May 14)] https://mccormickml.com/2019/05/14/BERT-word-embeddings-tutorial/\n",
        "\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qFoeorWj7EMX"
      },
      "source": [
        "### Info:\n",
        "* “The man was accused of robbing a bank.” “The man went fishing by the bank of the river.” Word2Vec would produce the same word embedding for the word “bank” in both sentences, while under BERT the word embedding for “bank” would be different for each sentence.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tNa-u6JV6IJJ"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fSHKjFn-9Kq3"
      },
      "source": [
        "import torch\n",
        "from transformers import BertTokenizer, BertModel"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Anqm2mG9Lg_"
      },
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T6jEnslt_Rrk",
        "outputId": "cef7f324-cc6d-4f8e-8afd-e9282317263c"
      },
      "source": [
        "# example = 'A tokenizer is in charge of preparing the inputs for a model.'\n",
        "# example = 'What meaning word \"embeddings\" has?'\n",
        "example = \"After stealing money from the bank vault, the bank robber was seen fishing on the Mississippi river bank.\"\n",
        "\n",
        "indexed_tokens = tokenizer.encode_plus(example, add_special_tokens=True)['input_ids']\n",
        "tokenized_text = [tokenizer.decode(w).replace(' ', '') for w in indexed_tokens]\n",
        "\n",
        "# print(tokenized_text)\n",
        "for tup in zip(tokenized_text, indexed_tokens):\n",
        "    print('{:<12} {:8,}'.format(tup[0], tup[1]))\n",
        "\n",
        "segments_ids = [1] * len(tokenized_text)\n",
        "print (segments_ids)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[CLS]             101\n",
            "after           2,044\n",
            "stealing       11,065\n",
            "money           2,769\n",
            "from            2,013\n",
            "the             1,996\n",
            "bank            2,924\n",
            "vault          11,632\n",
            ",               1,010\n",
            "the             1,996\n",
            "bank            2,924\n",
            "robber         27,307\n",
            "was             2,001\n",
            "seen            2,464\n",
            "fishing         5,645\n",
            "on              2,006\n",
            "the             1,996\n",
            "mississippi     5,900\n",
            "river           2,314\n",
            "bank            2,924\n",
            ".               1,012\n",
            "[SEP]             102\n",
            "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xoA_OUCrbqPN"
      },
      "source": [
        "tokens_tensor = torch.tensor([indexed_tokens])\n",
        "segments_tensors = torch.tensor([segments_ids])\n",
        "\n",
        "# print(tokens_tensor)\n",
        "# print(segments_tensors)\n",
        "\n",
        "model = BertModel.from_pretrained('bert-base-uncased', output_hidden_states = True)\n",
        "model.eval()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E6BwpbBQd0so",
        "outputId": "5548228e-32e4-465c-bb54-deb8612dcb80"
      },
      "source": [
        "with torch.no_grad():\n",
        "\n",
        "  outputs = model(tokens_tensor, segments_tensors)\n",
        "  hidden_states = outputs[2]\n",
        "  token_embeddings = torch.stack(hidden_states, dim=0)\n",
        "  token_embeddings = torch.squeeze(token_embeddings, dim=1)\n",
        "  token_embeddings = token_embeddings.permute(1,0,2)\n",
        "  print(token_embeddings.size())"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([22, 13, 768])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DdTyXw-4jayQ",
        "outputId": "6a7e5d84-1dcd-4cdf-a288-ec5e7dd5348f"
      },
      "source": [
        "# Word Vectors\n",
        "# ------------\n",
        "\n",
        "# Ex1:\n",
        "token_vecs_cat = []\n",
        "\n",
        "for token in token_embeddings:\n",
        "  # `token` is a [13 x 768] tensor\n",
        "\n",
        "  # Concatenate the vectors (that is, append them together) from the last four layers.\n",
        "  cat_vec = torch.cat((token[-1], token[-2], token[-3], token[-4]), dim=0)\n",
        "  \n",
        "  token_vecs_cat.append(cat_vec)\n",
        "\n",
        "print ('Shape is: %d x %d' % (len(token_vecs_cat), len(token_vecs_cat[0])))\n",
        "\n",
        "# Ex2:\n",
        "token_vecs_sum = []\n",
        "\n",
        "for token in token_embeddings:\n",
        "\n",
        "  # Sum the vectors from the last four layers.\n",
        "  sum_vec = torch.sum(token[-4:], dim=0)\n",
        "  \n",
        "  token_vecs_sum.append(sum_vec)\n",
        "\n",
        "print ('Shape is: %d x %d' % (len(token_vecs_sum), len(token_vecs_sum[0])))\n",
        "\n",
        "# Ex3:\n",
        "token_vecs_last = []\n",
        "\n",
        "for token in token_embeddings:\n",
        "\n",
        "  # Just vector from last layer for specific token.\n",
        "  last_vec = token[-1]\n",
        "\n",
        "  token_vecs_last.append(last_vec)\n",
        "\n",
        "print ('Shape is: %d x %d' % (len(token_vecs_last), len(token_vecs_last[0])))\n"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape is: 22 x 3072\n",
            "Shape is: 22 x 768\n",
            "Shape is: 22 x 768\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jNp2M8KMoC6l",
        "outputId": "1a608434-9592-4f69-e532-6c86b7d81b66"
      },
      "source": [
        "from scipy.spatial.distance import cosine\n",
        "\n",
        "diff_bank = 1 - cosine(token_vecs_last[10], token_vecs_last[19]) # \"bank robber\" vs \"river bank\",  one refers to the actual bank\n",
        "same_bank = 1 - cosine(token_vecs_last[10], token_vecs_last[6]) # \"bank robber\" vs \"bank vault\", both refers to the actual bank\n",
        "\n",
        "print('same bank', same_bank)\n",
        "print('diff_bank', diff_bank)\n"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "same bank 0.9527329206466675\n",
            "diff_bank 0.6978818774223328\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}